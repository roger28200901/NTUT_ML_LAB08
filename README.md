# NTUT_ML_LAB08

This is MountainCar Lab. For NTUT_ML class

## Reward Function

$R(s_{t}, a_{t}) = -1 + (p_{t} + 0.5)^2 + v_{t}^2 + R_{goal}$

備註：
MountainCar 環境中，位置 $p_t$ 的範圍是 [-1.2, 0.6]
當加上 0.5 的偏移量後

-   $(p_t + 0.5)^2$ 的範圍會變成：
-   最小值位置 (-1.2): $(-1.2 + 0.5)^2 = (-0.7)^2 = 0.49$
-   起始位置 (-0.5): $(-0.5 + 0.5)^2 = 0^2 = 0$
-   最大值位置 (0.6): $(0.6 + 0.5)^2 = 1.1^2 = 1.21$

解釋:

-   $R(s_{t}, a_{t})$ 是在時間 t 的狀態 $s_{t}$ 執行動作 $a_{t}$ 後獲得的總獎勵
-   $-1$ 是基礎懲罰
-   $p_{t}$ 是當前位置
-   $v_{t}$ 是當前速度
-   $R_{goal}$ 是到達目標的額外獎勵

這個獎勵函數的設計考慮了以下幾個方面：
基礎懲罰 (-1)：促使智能體盡快完成任務 2. 位置獎勵 $(p_{t} + 0.5)^2$：鼓勵小車向右移動
速度獎勵 $v_{t}^2$：鼓勵小車積累動能
目標獎勵 (100)：給予到達目標時的額外獎勵
